# Project Name

## Description
This project demonstrates how to locally run the Mistral7b model using Ollama and create a basic interface using Gradio to interact with the Language Model (LLM) and get response generated by LLM in your window.

## Installation
1. Clone the repository: `git clone https://github.com/Meet19aug/local-gpt.git`
2. install ollama from the website [ollama](https://ollama.com/) 
3. Run on terminal : ollama pull mistral

## Usage
1. Run the main script: `python main.py`
2. Open your web browser and navigate to `http://127.0.0.1:7860`
3. Enter your input in the provided text box and click the "Submit" button.
4. The response generated by the Language Model will be displayed on the screen.

## Contributing
Contributions are welcome! If you find any issues or have suggestions for improvements, please open an issue or submit a pull request.

## Acknowledgements
- [Ollama](https://github.com/ollama/ollama)
- [Gradio](https://github.com/gradio-app/gradio)
